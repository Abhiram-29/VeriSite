{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee5cbc24-c1a4-43a0-9d8a-7de79d657ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/abhir/VeriSite/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ce864c4-e537-49c0-8979-c0d3b1a07be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasMisleadingChars(url):\n",
    "  for char in url:\n",
    "    if not (char.isascii() or char.isspace()):\n",
    "      category = unicodedata.category(char)\n",
    "      if category.startswith(\"L\") and not unicodedata.combining(char):\n",
    "        return True\n",
    "  return  False\n",
    "    \n",
    "\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8964d7f-d777-4ee1-b725-38741bd835a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_url = \"https://ajax.googleapis.com/ajax/libs/jquery/1.5.1/jquery.min.js\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "459a20a9-b8f8-4447-a7d1-67d010a7bdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "data = pd.read_csv(\"train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6fd6e45-ac42-49aa-bd81-c2294be0cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data = data['url'].apply(lambda x: pd.Series(preprocess_url(x,False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ff7be12-7363-42f9-84e5-a00c32651a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain_length</th>\n",
       "      <th>subdomains</th>\n",
       "      <th>num_dots</th>\n",
       "      <th>num_equals</th>\n",
       "      <th>protocol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>319558.000000</td>\n",
       "      <td>319558.000000</td>\n",
       "      <td>319558.000000</td>\n",
       "      <td>319558.000000</td>\n",
       "      <td>319558.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>17.808977</td>\n",
       "      <td>1.693746</td>\n",
       "      <td>2.323531</td>\n",
       "      <td>0.725959</td>\n",
       "      <td>0.034388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.663161</td>\n",
       "      <td>1.053417</td>\n",
       "      <td>1.447766</td>\n",
       "      <td>1.667462</td>\n",
       "      <td>0.182224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>279.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       domain_length     subdomains       num_dots     num_equals  \\\n",
       "count  319558.000000  319558.000000  319558.000000  319558.000000   \n",
       "mean       17.808977       1.693746       2.323531       0.725959   \n",
       "std        10.663161       1.053417       1.447766       1.667462   \n",
       "min         1.000000       0.000000       0.000000       0.000000   \n",
       "25%        13.000000       1.000000       1.000000       0.000000   \n",
       "50%        16.000000       2.000000       2.000000       0.000000   \n",
       "75%        20.000000       2.000000       3.000000       0.000000   \n",
       "max       279.000000      27.000000      42.000000      51.000000   \n",
       "\n",
       "            protocol  \n",
       "count  319558.000000  \n",
       "mean        0.034388  \n",
       "std         0.182224  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c52546c5-7309-4ac0-836f-9a8a73d4dbbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain_length</th>\n",
       "      <th>subdomains</th>\n",
       "      <th>num_dots</th>\n",
       "      <th>num_equals</th>\n",
       "      <th>protocol</th>\n",
       "      <th>missing_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   domain_length  subdomains  num_dots  num_equals  protocol  missing_chars\n",
       "0             11           1         1           1         0          False\n",
       "1             26           2         3           0         0          False\n",
       "2              9           1         1           0         0          False\n",
       "3             20           2         4           0         0          False\n",
       "4             12           2         3           0         0          False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c376474a-875a-421a-9b70-c85170344aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e642b53-7839-4487-bc6a-924f75acf1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_url(url):\n",
    "    url = re.sub(r'https?://', '', url)\n",
    "    parts = url.split('/', 1)\n",
    "    domain = parts[0]\n",
    "    path = parts[1] if len(parts) > 1 else \"\"\n",
    "    text_rep = f\"{domain} {path.replace('/', ' ')}\"\n",
    "    \n",
    "    return text_rep\n",
    "\n",
    "def extract_url_features(url):\n",
    "    url = url.strip().lower()\n",
    "    protocol = 1 if urlparse(url).scheme == 'https' else 0\n",
    "    url = re.sub(r\"https?://\",\"\",url)\n",
    "    parts = url.split(\"/\",1)\n",
    "    domain = parts[0]\n",
    "    path = parts[1] if len(parts)>1 else \"\"\n",
    "\n",
    "    features = {\n",
    "        \"domain_length\" : len(domain),\n",
    "        \"subdomains\" : domain.count('.'),\n",
    "        'num_dots': url.count('.'),\n",
    "         'num_equals': url.count('='),\n",
    "         'protocol': protocol,\n",
    "        \"missing_chars\": hasMisleadingChars(url)\n",
    "    }\n",
    "   \n",
    "    return np.array(list(features.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "239ba712-b600-42c4-a1a2-5eb427ca8570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_deberta(urls, labels, model_name=\"microsoft/deberta-base\", epochs=3, batch_size=16):\n",
    "\n",
    "    processed_urls = [preprocess_url(url) for url in urls]\n",
    "    \n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        processed_urls, labels, test_size=0.15, stratify=labels, random_state=1\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=2\n",
    "    )\n",
    "\n",
    "    train_encodings = tokenizer(\n",
    "        train_texts, \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    val_encodings = tokenizer(\n",
    "        val_texts, \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    val_labels = torch.tensor(val_labels)\n",
    "    \n",
    "    train_dataset = TensorDataset(\n",
    "        train_encodings['input_ids'], \n",
    "        train_encodings['attention_mask'], \n",
    "        train_labels\n",
    "    )\n",
    "    \n",
    "    val_dataset = TensorDataset(\n",
    "        val_encodings['input_ids'], \n",
    "        val_encodings['attention_mask'], \n",
    "        val_labels\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size*2)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=0.05*total_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        val_accuracy = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[0].to(device)\n",
    "                attention_mask = batch[1].to(device)\n",
    "                labels = batch[2].to(device)\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "                val_accuracy += (predictions == labels).sum().item()\n",
    "        \n",
    "        val_accuracy /= len(val_dataset)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_train_loss:.4f} - Val Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_accuracies)\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('deberta_training_progress.png')\n",
    "    \n",
    "    model_path = \"./finetuned-deberta-url-classifier\"\n",
    "    model.save_pretrained(model_path)\n",
    "    tokenizer.save_pretrained(model_path)\n",
    "    \n",
    "    return model, tokenizer, model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95b32277-070e-40aa-8315-3a0d8cbe7009",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebertaFeatureExtractor:\n",
    "    def __init__(self, model_path):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModel.from_pretrained(model_path)  # Use base model, not classifier\n",
    "        self.model.eval()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def get_embeddings(self, urls, batch_size=32, max_length=128):\n",
    "        processed_urls = [preprocess_url(url) for url in urls]\n",
    "        \n",
    "        all_embeddings = []\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, len(processed_urls), batch_size):\n",
    "            batch_texts = processed_urls[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize\n",
    "            encodings = self.tokenizer(\n",
    "                batch_texts,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            input_ids = encodings['input_ids'].to(self.device)\n",
    "            attention_mask = encodings['attention_mask'].to(self.device)\n",
    "            \n",
    "            # Get embeddings\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "                # Use CLS token embedding as the URL embedding\n",
    "                embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            \n",
    "            all_embeddings.append(embeddings)\n",
    "        \n",
    "        return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e9217c0-613d-4c9e-ac6c-08abe378048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridURLClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=768, feature_dim=14, hidden_dim=256, dropout_rate=0.3):\n",
    "        super(HybridURLClassifier, self).__init__()\n",
    "        \n",
    "        self.feature_scaler = StandardScaler()\n",
    "        \n",
    "        self.fc1 = nn.Linear(embedding_dim + feature_dim, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, embeddings, features):\n",
    "        combined = torch.cat((embeddings, features), dim=1)\n",
    "        \n",
    "        # Forward pass\n",
    "        x = torch.relu(self.fc1(combined))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def fit_scaler(self, features):\n",
    "        \"\"\"Fit the StandardScaler to the engineered features\"\"\"\n",
    "        self.feature_scaler.fit(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365cbd88-c3e9-4468-8924-394a4fcfcbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hybrid_model(urls, labels, deberta_extractor, epochs=5, batch_size=32):\n",
    "    # Split data\n",
    "    train_urls, test_urls, train_labels, test_labels = train_test_split(\n",
    "        urls, labels, test_size=0.2, stratify=labels, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"Extracting DeBERTa embeddings...\")\n",
    "    train_embeddings = deberta_extractor.get_embeddings(train_urls)\n",
    "    test_embeddings = deberta_extractor.get_embeddings(test_urls)\n",
    "    \n",
    "    print(\"Extracting engineered features...\")\n",
    "    train_features = np.array([extract_url_features(url) for url in train_urls])\n",
    "    test_features = np.array([extract_url_features(url) for url in test_urls])\n",
    "    \n",
    "    embedding_dim = train_embeddings.shape[1]\n",
    "    feature_dim = train_features.shape[1]\n",
    "    \n",
    "    model = HybridURLClassifier(\n",
    "        embedding_dim=embedding_dim,\n",
    "        feature_dim=feature_dim,\n",
    "        hidden_dim=256,\n",
    "        dropout_rate=0.3\n",
    "    )\n",
    "    \n",
    "    # Fit scaler to training features\n",
    "    model.fit_scaler(train_features)\n",
    "    \n",
    "    # Scale features\n",
    "    train_features_scaled = model.feature_scaler.transform(train_features)\n",
    "    test_features_scaled = model.feature_scaler.transform(test_features)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    train_embeddings_tensor = torch.tensor(train_embeddings, dtype=torch.float32)\n",
    "    train_features_tensor = torch.tensor(train_features_scaled, dtype=torch.float32)\n",
    "    train_labels_tensor = torch.tensor(train_labels, dtype=torch.float32).view(-1, 1)\n",
    "    \n",
    "    test_embeddings_tensor = torch.tensor(test_embeddings, dtype=torch.float32)\n",
    "    test_features_tensor = torch.tensor(test_features_scaled, dtype=torch.float32)\n",
    "    test_labels_tensor = torch.tensor(test_labels, dtype=torch.float32).view(-1, 1)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    train_dataset = TensorDataset(\n",
    "        train_embeddings_tensor, train_features_tensor, train_labels_tensor\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            embeddings, features, labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(embeddings, features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_embeddings_tensor = test_embeddings_tensor.to(device)\n",
    "        test_features_tensor = test_features_tensor.to(device)\n",
    "        test_outputs = model(test_embeddings_tensor, test_features_tensor)\n",
    "        test_predictions = (test_outputs >= 0.5).float().cpu().numpy()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = (test_predictions.flatten() == test_labels).mean()\n",
    "    auc = roc_auc_score(test_labels, test_outputs.cpu().numpy())\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test AUC: {auc:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(test_labels, test_predictions.flatten()))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(test_labels, test_predictions.flatten())\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'hybrid_url_classifier.pth')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e050f09d-662a-4c52-b718-971b9614fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_malicious_urls(urls_data, labels_data, deberta_model_name=\"microsoft/deberta-base\"):\n",
    "    \"\"\"Complete pipeline for malicious URL detection\"\"\"\n",
    "    print(\"Step 1: Fine-tuning DeBERTa model...\")\n",
    "    deberta_model, tokenizer, model_path = finetune_deberta(\n",
    "        urls_data, labels_data, model_name=deberta_model_name, epochs=3\n",
    "    )\n",
    "    \n",
    "    print(\"\\nStep 2: Creating DeBERTa feature extractor...\")\n",
    "    deberta_extractor = DebertaFeatureExtractor(model_path)\n",
    "    \n",
    "    print(\"\\nStep 3: Training hybrid neural network...\")\n",
    "    hybrid_model = train_hybrid_model(\n",
    "        urls_data, labels_data, deberta_extractor, epochs=5\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining complete! Models saved to:\")\n",
    "    print(f\"- DeBERTa model: {model_path}\")\n",
    "    print(\"- Hybrid model: hybrid_url_classifier.pth\")\n",
    "    return deberta_extractor, hybrid_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da4237-98f4-47f1-b386-ada7580ec8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your dataset\n",
    "    # df = pd.read_csv('url_dataset.csv')\n",
    "    # urls = df['url'].tolist()\n",
    "    # labels = df['is_malicious'].tolist()\n",
    "    \n",
    "    # Run the full pipeline\n",
    "    # deberta_extractor, hybrid_model = detect_malicious_urls(urls, labels)\n",
    "    \n",
    "    # For inference on new URLs\n",
    "    def predict_url(url, deberta_extractor, hybrid_model):\n",
    "        # Extract DeBERTa embedding\n",
    "        embedding = deberta_extractor.get_embeddings([url])\n",
    "        embedding_tensor = torch.tensor(embedding, dtype=torch.float32)\n",
    "        \n",
    "        # Extract and scale engineered features\n",
    "        features = extract_url_features(url).reshape(1, -1)\n",
    "        features_scaled = hybrid_model.feature_scaler.transform(features)\n",
    "        features_tensor = torch.tensor(features_scaled, dtype=torch.float32)\n",
    "        \n",
    "        # Make prediction\n",
    "        device = next(hybrid_model.parameters()).device\n",
    "        embedding_tensor = embedding_tensor.to(device)\n",
    "        features_tensor = features_tensor.to(device)\n",
    "        \n",
    "        hybrid_model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = hybrid_model(embedding_tensor, features_tensor)\n",
    "        \n",
    "        probability = output.item()\n",
    "        prediction = 1 if probability >= 0.5 else 0\n",
    "        \n",
    "        return prediction, probability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
